{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note about define custom apachebeam template and run that template**\n",
    "- content: build custom apache beam data pipeline flex template and run it with\n",
    "- reference link: https://github.com/GoogleCloudPlatform/python-docs-samples/tree/473749daea6b1f1d5a6a8826093a970f03ce0517/dataflow/flex-templates/streaming_beam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list google cloud services with --filter flag, filter services with name contain [app_name]\n",
    "#gcloud services list --available take too long\n",
    "!gcloud services list --available --filter=\"name~[app_name]\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#enable api service: app engine, cloud scheduler, \n",
    "gcloud services enable $service_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create cloud storage bucket with command: gsutil mb\n",
    "export BUCKET=\"your-gcs-bucket\"\n",
    "gsutil mb gs://$BUCKET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create topic and subscription with command: gcloud pubsub\n",
    "export TOPIC=\"messages\"\n",
    "export SUBSCRIPTION=\"$TOPIC\"\n",
    "\n",
    "gcloud pubsub topics create $TOPIC\n",
    "gcloud pubsub subscriptions create --topic $TOPIC $SUBSCRIPTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create scheduled job to push data to pubsub with job name is positive-rating-publisher, schedule, topic, message-body\n",
    "gcloud scheduler jobs create pubsub positive-ratings-publisher \\\n",
    "  --schedule=\"* * * * *\" \\\n",
    "  --topic=\"$TOPIC\" \\\n",
    "  --message-body='{\"url\": \"https://beam.apache.org/\", \"review\": \"positive\"}'\n",
    "#trigger job with run command\n",
    "gcloud scheduler jobs run positive-ratings-publisher\n",
    "\n",
    "#create scheduled job which push message to\n",
    "gcloud scheduler jobs create pubsub negative-ratings-publisher \\\n",
    "  --schedule=\"*/2 * * * *\" \\\n",
    "  --topic=\"$TOPIC\" \\\n",
    "  --message-body='{\"url\": \"https://beam.apache.org/\", \"review\": \"negative\"}'\n",
    "#trigger the job\n",
    "gcloud scheduler jobs run negative-ratings-publisher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make bigquery dataset with bq mk\n",
    "#bq cli reference: https://cloud.google.com/bigquery/docs/reference/bq-cli-reference\n",
    "export PROJECT=\"$(gcloud config get-value project)\"\n",
    "export DATASET=\"beam_samples\"\n",
    "export TABLE=\"streaming_beam\"\n",
    "\n",
    "bq mk --dataset \"$PROJECT:$DATASET\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build apache beam container image with cloud build command\n",
    "\n",
    "#set config to use kaniki tools as cache tools\n",
    "gcloud config set builds/use_kaniko True\n",
    "#build docker container and submit it to container registry\n",
    "gcloud builds submit --tag \"$dataflow_image\" ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build flex template with gcloud dataflow flex-template build command. this command will build template from docker image and store it as a json file in at cloud storage\n",
    "#metadata.json reference link: https://cloud.google.com/dataflow/docs/guides/templates/using-flex-templates#metadata\n",
    "#metadata.json is a file which we define template meta information like name, description, parameters\n",
    "gcloud dataflow flex-template build $template_path \\\n",
    "  --image \"$dataflow_image\" \\\n",
    "  --sdk-language \"PYTHON\" \\\n",
    "  --metadata-file \"metadata.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run dataflow flex template with command: gcloud dataflow flex-template run\n",
    "#in our case, we want to run flex template from cloud function\n",
    "export REGION=\"us-central1\"\n",
    "\n",
    "# Run the Flex Template.\n",
    "gcloud dataflow flex-template run \"streaming-beam-`date +%Y%m%d-%H%M%S`\" \\\n",
    "    --template-file-gcs-location \"$TEMPLATE_PATH\" \\\n",
    "    --parameters input_subscription=\"projects/$PROJECT/subscriptions/$SUBSCRIPTION\" \\\n",
    "    --parameters output_table=\"$PROJECT:$DATASET.$TABLE\" \\\n",
    "    --region \"$REGION\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#copy test_data.csv to trigger bucket\n",
    "gsutil cp ./data/test_data.csv gs://fce2845e810918fb-gcf-trigger-bucket/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using Individual service account for your functions**\n",
    "- to deploy cloud function with individual service account. Cloud functions will use indivisual service-account for authentication\n",
    "- we can use individual service account at deployment\n",
    "- reference link: https://cloud.google.com/functions/docs/securing/function-identity#individual "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Securities and Permissions for pipeline on Google Cloud**\n",
    "- to understand securities and permission for dataflow pipeline on google cloud\n",
    "- dataflow pipeline have two types of service account\n",
    "    - dataflow service account:\n",
    "        - this service account is used as a resource manager for dataflow pipeline. For example create vm and assign job to vm workder\n",
    "        - this account is created and managed by google. Do not touch it.\n",
    "        - format of the serive account email: service-<project-number>@dataflow-service-producer-prod.iam.gserviceaccount.com\n",
    "        - reference link: https://cloud.google.com/dataflow/docs/concepts/security-and-permissions#df-service-account\n",
    "    - worker service account\n",
    "        - worker service account is service account used by worker (compute engine vm) to access your data pipeline files and other resources\n",
    "        - worker service account must have two below role to create, run and exame job:\n",
    "            - roles/dataflow.admin\n",
    "            - roles/dataflow.worker\n",
    "        - default workder service account\n",
    "            - by default dataflow worker will use compute engine default sevice account for authentication\n",
    "            - compute engine default service account will be auto-created when you enable compute engine api.\n",
    "            - compute engine default service account format: <project-number>-compute@developer.gserviceaccount.com\n",
    "            - compute engine default service account have some predefine permission which make authentication eaiser but it is recommented to use custom service account in production for more detail access control.\n",
    "        - specify a user-managed worker service account\n",
    "            - we can specify custom worker service account on job deployment. In our case is api call\n",
    "            - for more detail for roles and permmission for the user-managed worker service account take a look at reference link: https://cloud.google.com/dataflow/docs/concepts/security-and-permissions#worker-service-account "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#goal of today:\n",
    "    #specify temp bucket for dataflow flex template. this should solve permission denied for create temp directory\n",
    "    #grant access to bucket and job for default worker service account\n",
    "    #for grant access to bucket take a look at reference link: https://cloud.google.com/dataflow/docs/concepts/security-and-permissions#accessing_gcs\n",
    "\n",
    "#assign compute engine default service account (dataflow worker service account) using gutils acl:\n",
    "    # acl (access control list): is a set of command line tool to grant access to specific bucket to specific account\n",
    "    # reference for gsutil acl command: https://cloud.google.com/storage/docs/gsutil/commands/acl\n",
    "\n",
    "#gran worker service account to temp, src, dest\n",
    "gsutil acl ch -u \"$worker_sa:OWNER\" $bucket_one\n",
    "gsutil acl ch -u \"$worker_sa:OWNER\" $bucket_two\n",
    "gsutil acl ch -u \"$worker_sa:OWNER\" $bucket_three"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Understand stagging and temp location**\n",
    "- reference link: https://cloud.google.com/dataflow/docs/guides/templates/configuring-flex-templates#understand_staging_location_and_temp_location\n",
    "- stagging location is where files is written during stagging process\n",
    "- temp location is where fiesl is written during execution step\n",
    "- we can specify stagging location and temp location when create flex template job both with cli or api call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#copy test_data.csv to trigger bucket\n",
    "gsutil cp ./data/test_data.csv gs://fce2845e810918fb-gcf-trigger-bucket/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "googleapiclient.errors.HttpError: <HttpError 403 when requesting https://dataflow.googleapis.com/v1b3/projects/airflow-gke-338120-352104/locations/asia-southeast1/flexTemplates:launch?alt=json returned \"(c3d2125d4ad0a248): Current user cannot act as service account 149838564778-compute@developer.gserviceaccount.com. Enforced by Org Policy constraint constraints/dataflow.enforceComputeDefaultServiceAccountCheck. https://cloud.google.com/iam/docs/service-accounts-actas Causes: (c3d2125d4ad0ac16): Current user cannot act as service account 149838564778-compute@developer.gserviceaccount.com. Please grant your user account one of [Owner, Editor, Service Account Actor] roles, or any other role that includes the iam.serviceAccounts.actAs permission. See https://cloud.google.com/iam/docs/service-accounts-actas for additional details.\". Details: \"(c3d2125d4ad0a248): Current user cannot act as service account 149838564778-compute@developer.gserviceaccount.com. Enforced by Org Policy constraint constraints/dataflow.enforceComputeDefaultServiceAccountCheck. https://cloud.google.com/iam/docs/service-accounts-actas Causes: (c3d2125d4ad0ac16): Current user cannot act as service account 149838564778-compute@developer.gserviceaccount.com. Please grant your user account one of [Owner, Editor, Service Account Actor] roles, or any other role that includes the iam.serviceAccounts.actAs permission. See https://cloud.google.com/iam/docs/service-accounts-actas for additional details.\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get service account \n",
    "gcloud iam service-accounts get-iam-policy $worker_sa \\\n",
    "    --format=json > policy.json\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#what is the error:\n",
    "    #can not set policy for resource\n",
    "    #because do not have Permission iam.serviceAccounts.setIamPolicy\n",
    "│ Error: Error setting IAM policy for service account 'projects/airflow-gke-338120-352104/serviceAccounts/149838564778-compute@developer.gserviceaccount.com': googleapi: Error 403: Permission iam.serviceAccounts.setIamPolicy is required to perform this operation on service account projects/airflow-gke-338120-352104/serviceAccounts/149838564778-compute@developer.gserviceaccount.com., forbidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gran roles/iam.serviceAccountUser to worker_sa\n",
    "!gcloud iam service-accounts add-iam-policy-binding $worker_sa \\\n",
    "    --member=\"serviceAccount:${worker_sa}\" --role=\"roles/iam.serviceAccountUser\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gran roles/iam.serviceAccountUser to worker_sa\n",
    "!gcloud iam service-accounts add-iam-policy-binding $worker_sa \\\n",
    "    --member=\"serviceAccount:${function_sa}\" --role=\"roles/iam.serviceAccountUser\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#copy file from local to gs bucket to trigger dataflow\n",
    "gsutil cp ./data/test_data.csv gs://fce2845e810918fb-gcf-trigger-bucket/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "215640d84fea2d6a1578a868d021d57d26fe3fa314db54416f0ba6d81a3e2987"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
